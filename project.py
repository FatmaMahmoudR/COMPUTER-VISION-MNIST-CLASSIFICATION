# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13vSSBjNDYX6yMzGfRURCKKHIRQDNGj09
"""

import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

# libraries for ANN
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.metrics import confusion_matrix, classification_report
from random import choice, uniform
from tensorflow.keras.models import load_model
import joblib


##1) Load the dataset and perform initial data exploration.

data =pd.read_csv("mnist_train.csv")
# Display the first 5 rows of the dataset
print(" Data Head: \n",data.head())

# Get an overview of the dataset including column names, data types, and non-null counts
print("Data Information: \n",data.info())

##2) Identify the number of unique classes
unique_classes = data.iloc[:, 0].nunique()
print("Number of Unique Classes:", unique_classes)

##3) Identify the number of features
print("Number of Features:", data.shape[1] - 1)

##4) handle missing values
print("Missing Values:\n", data.isnull().sum())



# Drop rows with any missing values
data.dropna(axis=0, inplace=True)

# Drop columns with any missing values
data.dropna(axis=1, inplace=True)

print("After Handling Missing Values:\n", data.isnull().sum())

##5) Normalize each image by dividing each pixel by 255
data.iloc[:, 1:] = data.iloc[:, 1:] / 255.0


##6) Split the dataset into features (X) and labels (y)
X = data.iloc[:, 1:]
y = data.iloc[:, 0]

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Verify the shapes of the sets
print("Training set shape:", X_train.shape, y_train.shape)
print("Validation set shape:", X_val.shape, y_val.shape)

##7)resizing the images to 28x28 pixels
# Assuming data contains the pixel values, and each row represents an image
# X_train, X_val, or any other DataFrame with pixel values can be used

# Function to resize an image to 28x28
# def resize_image(image_array):
#     image = Image.fromarray((image_array * 255).astype('uint8').reshape(28, 28))
#     resized_image = image.resize((28, 28))
#     return np.array(resized_image) / 255.0

# Choose some random indices to visualize
indices_to_visualize = np.random.choice(len(X_train), 5, replace=False)

# Visualize original and resized images
for i in indices_to_visualize:
    original_image = X_train.iloc[i, :].values.reshape(28, 28)
    # resized_image = resize_image(X_train.iloc[i, :].values)

    plt.figure(figsize=(8, 4))

    plt.subplot(1, 2, 1)
    plt.imshow(original_image, cmap='gray')
    plt.title('Original Image')

    # plt.subplot(1, 2, 2)
    # plt.imshow(resized_image, cmap='gray')
    # plt.title('Resized Image (28x28)')

    plt.show()

#-------------- Initial Experiment (K-NN) --------------
knn_model = KNeighborsClassifier()
param_grid = {
    'n_neighbors': [3, 5],
    'weights': ['distance'],
}

# # Perform grid search using cross-validation
grid_search = GridSearchCV(knn_model, param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

print("Best Hyperparameters:", grid_search.best_params_)

best_knn_model = grid_search.best_estimator_
best_knn_model.fit(X_train, y_train)

knn_predictions = best_knn_model.predict(X_val)

accuracy = accuracy_score(y_val, knn_predictions)
print("K-NN Accuracy:", accuracy)

# best_knn_model.save('best_knn_model.h5')

#--------------- Subsequent Experiment (ANN) --------------
ANN_model1 = models.Sequential([
    layers.Flatten(input_shape=(28, 28)),
    layers.Dense(25, activation='relu'),
    layers.Dense(15, activation='relu'),
    layers.Dense(10, activation='linear')
])

# Compile the model
ANN_model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
# Train the model
ANN_model1.fit(X_train.values.reshape(-1, 28, 28), y_train.values, epochs=5, validation_data=(X_val.values.reshape(-1, 28, 28), y_val.values))

# Evaluate the model on the validation set
ANN_model1_accuracy = ANN_model1.evaluate(X_val.values.reshape(-1, 28, 28), y_val.values)[1]
print("ANN Model 1 Accuracy:", ANN_model1_accuracy)

ANN_model2 = models.Sequential([
    layers.Flatten(input_shape=(28, 28)),
    layers.Dense(128, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Compile the model
ANN_model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
# Train the model
ANN_model2.fit(X_train.values.reshape(-1, 28, 28), y_train.values, epochs=5, validation_data=(X_val.values.reshape(-1, 28, 28), y_val.values))

# Evaluate the model on the validation set
ANN_model2_accuracy = ANN_model2.evaluate(X_val.values.reshape(-1, 28, 28), y_val.values)[1]
print("ANN Model 2 Accuracy:", ANN_model2_accuracy)


best_ANN_model = None
best_ANN_accuracy = 0

if ANN_model1_accuracy > ANN_model2_accuracy:
    best_ann_accuracy = ANN_model1_accuracy
    best_ann_model = ANN_model1
else:
    best_ann_accuracy = ANN_model2_accuracy
    best_ann_model = ANN_model2

print("Best ANN Model Accuracy:", best_ann_accuracy)

best_ann_model.save('best_ann_model.h5')

if accuracy >= best_ann_accuracy:
    best_model = best_knn_model
    model_type = "K-NN"
else:
    best_model = best_ann_model
    model_type = "ANN"
print(f"\nThe best model is {model_type} with an accuracy of {max(accuracy, best_ann_accuracy)}")

# Load the testing data from mnist_test.csv
test_data = pd.read_csv("mnist_test.csv")

# Normalize the pixel values
test_data.iloc[:, 1:] = test_data.iloc[:, 1:] / 255.0
# Split the testing data into features (X_test) and labels (y_test)
X_test = test_data.iloc[:, 1:]
y_test = test_data.iloc[:, 0]

if model_type == "K-NN":
    #get confusion matrix for best knn model
    conf_matrix = confusion_matrix(y_val, best_knn_model.predict(X_val))
    #print confusion matrix
    print("Confusion Matrix (K-NN):")
    print(conf_matrix)
    
    # Save the best K-NN model using joblib
    joblib.dump(best_knn_model, 'best_knn_model.joblib')

    # Reload the best K-NN model from the saved file
    loaded_knn_model = joblib.load('best_knn_model.joblib')

    # Evaluate the best K-NN model on the testing set
    y_pred_knn = loaded_knn_model.predict(X_test)
    print("Predictions using the loaded best K-NN model:")
    print(y_pred_knn)
else:
    # Load the best ANN model
    loaded_ann_model = load_model('best_ann_model.h5')

    # Make predictions on the entire dataset
    predictions_ann = loaded_ann_model.predict(X_val.values.reshape(-1, 28, 28))

    # Convert probability scores to class predictions
    predictions_ann_classes = tf.argmax(predictions_ann, axis=1).numpy()

    #get confusion matrix
    conf_matrix_ann = confusion_matrix(y_val, predictions_ann_classes)

    # Print the confusion matrix
    print("Confusion Matrix (ANN):")
    print(conf_matrix_ann)

    # Make predictions on the testing set
    predictions_annn = loaded_ann_model.predict(X_test.values.reshape(-1, 28, 28))

    if predictions_annn.shape[1] == 1:
    # If you have a single output neuron with softmax activation
     predictions_annn_classes = tf.argmax(predictions_ann, axis=1).numpy()
    else:
    # If you have multiple output neurons with softmax activation
     predictions_annn_classes = tf.argmax(predictions_ann, axis=-1).numpy()

    print("Predictions using the loaded ANN model:")
    print(predictions_annn_classes)